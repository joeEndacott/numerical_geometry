{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cefe33d9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Learn the basics - PyTorch tutorial\n",
    "\n",
    "This notebook contains notes and code from the [Learn the Basics](https://docs.pytorch.org/tutorials/beginner/basics/intro.html) PyTorch tutorial from the official PyTorch documentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf87068",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Table of Contents**\n",
    "\n",
    "### [**0 - Quickstart**](#0---quickstart)\n",
    "\n",
    "- [0.1 - Working with data](#01---working-with-data)\n",
    "- [0.2 - Creating models](#02---creating-models)\n",
    "- [0.3 - Optimizing the model parameters](#03---optimizing-the-model-parameters)\n",
    "- [0.4 - Saving and loading models](#04---saving-and-loading-models)\n",
    "\n",
    "### [**1 - Tensors**](#1---tensors)\n",
    "\n",
    "- [1.1 - Initializing a tensor](#11---initializing-a-tensor)\n",
    "- [1.2 - Attributes of a tensor](#12---attributes-of-a-tensor)\n",
    "- [1.3 - Operations on tensors](#13---operations-on-tensors)\n",
    "- [1.4 - Bridge with NumPy](#14---bridge-with-numpy)\n",
    "\n",
    "### [**2 - `Datasets` and `DataLoaders`**](#2---datasets-and-dataloaders)\n",
    "\n",
    "- [2.1 - Loading a dataset](#21---loading-a-dataset)\n",
    "- [2.2 - Iterating and visualizing a dataset](#22---iterating-and-visualizing-a-dataset)\n",
    "- [2.3 - Creating a custom dataset for your files](#23---creating-a-custom-dataset-for-your-files)\n",
    "- [2.4 - Preparing your data for training with `DataLoaders`](#24---preparing-your-data-for-training-with-dataloaders)\n",
    "\n",
    "### [**3 - Transforms**](#3---transforms)\n",
    "\n",
    "### [**4 - Building neural networks**](#4---building-neural-networks)\n",
    "\n",
    "- [4.1 - Getting a device for training](#41---getting-a-device-for-training)\n",
    "- [4.2 - Defining the class](#42---defining-the-class)\n",
    "- [4.3 - Model layers](#43---model-layers)\n",
    "- [4.4 - `nn.Softmax`](#44---nnsoftmax)\n",
    "- [4.5 - Model parameters](#45---model-parameters)\n",
    "\n",
    "### [**5 - Automatic differentiation with `torch.autograd`**](#5---automatic-differentiation-with-torchautograd)\n",
    "\n",
    "- [5.1 - Tensors, functions and computational graphs](#51---tensors-functions-and-computational-graphs)\n",
    "- [5.2 - Computing gradients](#52---computing-gradients)\n",
    "\n",
    "### [**6 - Optimizing model parameters**](#6---optimizing-model-parameters)\n",
    "\n",
    "- [6.1 - Hyperparameters](#61---hyperparameters)\n",
    "- [6.2 - The optimization loop](#62---the-optimization-loop)\n",
    "- [6.3 - Loss functions](#63---loss-functions)\n",
    "- [6.4 - Optimizers](#64---optimizers)\n",
    "- [6.5 - Full implementation](#65---full-implementation)\n",
    "\n",
    "### [**7 - Saving and loading models**](#7---saving-and-loading-models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d42b3c0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 0 - Quickstart\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb11de4",
   "metadata": {},
   "source": [
    "### 0.1 - Working with data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1fc9d5",
   "metadata": {},
   "source": [
    "Import the required modules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3147a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6affd52f",
   "metadata": {},
   "source": [
    "Download training and test data from the FashionMNIST dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2ec8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4659a406",
   "metadata": {},
   "source": [
    "Initialize data loaders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39af8ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593d1f5e",
   "metadata": {},
   "source": [
    "### 0.2 - Creating Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b70ab1",
   "metadata": {},
   "source": [
    "We can define a neural network in PyTorch by creating a class which inherits from `nn.Module`. Layers of the network are defined in the `__init__` function. We specify how data passes through the network in the `forward` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0006c1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device to MPS if available, otherwise CPU.\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using {device} device.\")\n",
    "\n",
    "\n",
    "# Define the model.\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd1786c",
   "metadata": {},
   "source": [
    "### 0.3 - Optimizing the model parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e5265b",
   "metadata": {},
   "source": [
    "To train a model, we need a loss function and an optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf90a861",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb9333a",
   "metadata": {},
   "source": [
    "In each training loop, the model makes predictions on the training dataset, and backpropagates the prediction error to adjust the model's parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5e09f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_function, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (x, y) in enumerate(dataloader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        prediction = model(x)\n",
    "        # Compute the error in the prediction.\n",
    "        loss = loss_function(prediction, y)\n",
    "\n",
    "        # Backpropagate the prediction error.\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da038ce",
   "metadata": {},
   "source": [
    "We can check the model's performance against the test dataset to ensure that it is learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a180b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_function):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            prediction = model(x)\n",
    "            test_loss += loss_function(prediction, y).item()\n",
    "            correct += (prediction.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Accuracy: {(100*correct):>0.1f}%; average loss: {test_loss:>8f}. \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f11e9e",
   "metadata": {},
   "source": [
    "The training process is conducted over several epochs. We will train the model, and print the model's accuracy and loss at each epoch. We want the accuracy to increase and the loss to decrease with each epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59edb10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n----------------------------------------\")\n",
    "    train(train_dataloader, model, loss_function, optimizer)\n",
    "    test(test_dataloader, model, loss_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2d703e",
   "metadata": {},
   "source": [
    "### 0.4 - Saving and loading models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c887ac",
   "metadata": {},
   "source": [
    "We can save a model by serializing the internal state dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff97b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"model.pth\"\n",
    "torch.save(model.state_dict(), filename)\n",
    "print(f\"Saved PyTorch model state to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428a59e1",
   "metadata": {},
   "source": [
    "We can load a model by re-creating the model structure, and loading the state dictionary into it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50279b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load(\"model.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ad4ef9",
   "metadata": {},
   "source": [
    "We can now use the model to make predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be56489d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for i in range(5):\n",
    "    x, y = test_data[i][0], test_data[i][1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x = x.to(device)\n",
    "        prediction = model(x)\n",
    "        predicted, actual = classes[prediction[0].argmax(0)], classes[y]\n",
    "        print(f\"Predicted: '{predicted}'; actual: '{actual}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c682b4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1 - Tensors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64574a41",
   "metadata": {},
   "source": [
    "Tensors in PyTorch encode the inputs and outputs of a model, as well as the model's parameters. Unlike NumPy's `ndarray`, PyTorch's `Tensor` can run on a GPU. Tensors are optimized for automatic differentiation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993baa9c",
   "metadata": {},
   "source": [
    "Import the required modules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afa7ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadd21e0",
   "metadata": {},
   "source": [
    "### 1.1 - Initializing a tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b941bf9",
   "metadata": {},
   "source": [
    "Tensors can be initialized directly from data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37164372",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 2], [3, 4]]\n",
    "x_data = torch.tensor(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a5b1c8",
   "metadata": {},
   "source": [
    "Tensors can be created from NumPy arrays.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c04b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data_np = np.array(data)\n",
    "x_data_from_np = torch.from_numpy(x_data_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1877dc72",
   "metadata": {},
   "source": [
    "Tensors can be created from another tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef4a882",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ones = torch.ones_like(x_data)\n",
    "print(f\"Ones tensor: \\n{x_ones}\\n\")\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float)\n",
    "print(f\"Random tensor: \\n{x_rand}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400d1c71",
   "metadata": {},
   "source": [
    "Tensors can be created by specifying their shape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7f524b",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (\n",
    "    2,\n",
    "    3,\n",
    ")\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random tensor: \\n{rand_tensor}\\n\")\n",
    "print(f\"Ones tensor: \\n{ones_tensor}\\n\")\n",
    "print(f\"Zeros tensor: \\n{zeros_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d54068c",
   "metadata": {},
   "source": [
    "### 1.2 - Attributes of a tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a5dc2f",
   "metadata": {},
   "source": [
    "Tensors have different attributes which can be accessed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbe0f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (\n",
    "    3,\n",
    "    4,\n",
    ")\n",
    "tensor = torch.rand(shape)\n",
    "print(f\"Tensor: \\n{tensor}\\n\")\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}.\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}.\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e550966",
   "metadata": {},
   "source": [
    "### 1.3 - Operations on tensors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aeeecd2",
   "metadata": {},
   "source": [
    "Tensor operations can be run on the CPU or the GPU. By default, tensors are created on the CPU, and we need to move tensors to the GPU using the `.to` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0a5c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device to MPS if available, otherwise CPU.\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using {device} device.\")\n",
    "\n",
    "tensor = tensor.to(device)\n",
    "print(tensor.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab71a2f",
   "metadata": {},
   "source": [
    "### 1.4 - Bridge with NumPy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b91e75",
   "metadata": {},
   "source": [
    "Tensors on the CPU and NumPy arrays can share their memory locations, and so changing one will affect the other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fba9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_array = torch.ones(5)\n",
    "np_array = torch_array.numpy()\n",
    "print(f\"Torch array: {torch_array}.\")\n",
    "print(f\"NumPy array: {np_array}.\")\n",
    "\n",
    "torch_array += 1\n",
    "print(f\"Torch array: {torch_array}.\")\n",
    "print(f\"NumPy array: {np_array}.\")\n",
    "\n",
    "np_array += 1\n",
    "print(f\"Torch array: {torch_array}.\")\n",
    "print(f\"NumPy array: {np_array}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6c1f36",
   "metadata": {},
   "source": [
    "We can make a PyTorch `Tensor` from a NumPy `ndarray`. Similarly, these two objects will share their memory locations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcac2f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_array = np.ones(5)\n",
    "torch_array = torch.from_numpy(np_array)\n",
    "print(f\"Torch array: {torch_array}.\")\n",
    "print(f\"NumPy array: {np_array}.\")\n",
    "\n",
    "torch_array += 1\n",
    "print(f\"Torch array: {torch_array}.\")\n",
    "print(f\"NumPy array: {np_array}.\")\n",
    "\n",
    "np_array += 1\n",
    "print(f\"Torch array: {torch_array}.\")\n",
    "print(f\"NumPy array: {np_array}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def7e3f4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2 - `Datasets` and `DataLoaders`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda84d5c",
   "metadata": {},
   "source": [
    "PyTorch provides two data primitives, `torch.utils.data.DataLoader` and `torch.utils.data.Dataset`, that allow you to use pre-loaded datasets and your own data. `Dataset` stores the samples and labels, and `DataLoader` wraps an iterable around the `Dataset` so that you can easily access samples and labels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3652a2b6",
   "metadata": {},
   "source": [
    "Import the required modules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfecbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f452c19d",
   "metadata": {},
   "source": [
    "### 2.1 - Loading a dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8de89c",
   "metadata": {},
   "source": [
    "We will load the Fashion-MNIST dataset from TorchVision, which contains 60,000 training examples and 10,000 test examples. Each example is a 28x28 grayscale image, with a label from one of 10 classes.\n",
    "\n",
    "`root` is the path where the train/test data is stored; `train` specifies training or test dataset; `download` specifies whether or not to download data from the internet; `transform` specifies the feature and label transformations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71185fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\", train=True, download=True, transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\", train=False, download=True, transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc4dd12",
   "metadata": {},
   "source": [
    "### 2.2 - Iterating and visualizing a dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c461f6",
   "metadata": {},
   "source": [
    "We will use Matplotlib to visualize some samples from our training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1e8df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ace7d6c",
   "metadata": {},
   "source": [
    "### 2.3 - Creating a custom dataset for your files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afc0cff",
   "metadata": {},
   "source": [
    "Below is an example of a custom dataset class; the FashionMNIST images are stored in a directory `img_dir`, and their labels are stored separately in a CSV file `annotations_file`.\n",
    "\n",
    "A custom dataset class must implement three functions: `__init__`, `__len__`, and `__getitem__`:\n",
    "\n",
    "- The `__init__` function is run when initializing the dataset object.\n",
    "- The `__len__` function returns the number of samples in the dataset.\n",
    "- The `__getitem__` function loads and returns a sample from the dataset at a given index, `idx`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403ec691",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import decode_image\n",
    "\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, annotations_file, img_dir, transform=None, target_transform=None\n",
    "    ):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = decode_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2f5380",
   "metadata": {},
   "source": [
    "### 2.4 - Preparing your data for training with `DataLoaders`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab2067b",
   "metadata": {},
   "source": [
    "The `Dataset` retrieves features and labels from a dataset one sample at a time. When training a model, we want to pass samples in minibatches, reshuffle the data at each epoch, and use Python's `multiprocessing` to speed up data retrieval. `DataLoader` abstracts this complexity away for us.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a09bb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29b6e01",
   "metadata": {},
   "source": [
    "Once we have loaded a dataset into the `DataLoader`, we can iterate through the dataset as needed. Each iteration below returns a batch of `train_features` and `train_labels`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b748360",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002b7d2b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3 - Transforms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078b07e5",
   "metadata": {},
   "source": [
    "Transforms manipulate the data to make it suitable for training. TorchVision datasets have two parameters; `transform` modifies the features, and `target_transform` modifies the label.\n",
    "\n",
    "For training, we need to convert the features (images) of the FashionMNIST dataset to tensors, and the labels (integers) to one-hot encoded tensors. We do these transformations using `ToTensor` and `Lambda`. `ToTensor()` converts a PIL image, or a NumPy `ndarray`, to a `FloatTensor`. Lambda transforms apply any user-defined lambda function. Below, we define a function to turn the integer into a one-hot encoded tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bc0ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "dataset = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=Lambda(\n",
    "        lambda y: torch.zeros(10, dtype=torch.float).scatter_(\n",
    "            0, torch.tensor(y), value=1\n",
    "        )\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc0fe9e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4 - Building neural networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb68c392",
   "metadata": {},
   "source": [
    "Neural networks are comprised of layers/modules that perform operations on data. The `torch.nn` module provides all of the building blocks necessary to build a neural network. In this section, we will build a neural network to classify images in the FashionMNIST dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2ded60",
   "metadata": {},
   "source": [
    "Import required modules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd6547b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75950c99",
   "metadata": {},
   "source": [
    "### 4.1 - Getting a device for training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7601e010",
   "metadata": {},
   "source": [
    "We want to train our model on the GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a9a50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device to MPS if available, otherwise CPU.\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using {device} device.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5099e6",
   "metadata": {},
   "source": [
    "### 4.2 - Defining the class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57f2355",
   "metadata": {},
   "source": [
    "We define the neural network by subclassing `nn.Module`, and we initialize the neural network layers in `__init__`. Every `nn.Module` subclass implements operations on data in the `forward` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98199605",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6c61d6",
   "metadata": {},
   "source": [
    "We will now create an instance of `NeuralNetwork` and move it to the `device`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c565916",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0bbc72",
   "metadata": {},
   "source": [
    "To use the model, we pass it input data. This executes `forward`, as well as background operations. If we pass the output from the model through the `nn.Softmax` module, we can get predicted probabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18601e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = torch.rand(1, 28, 28).to(device)\n",
    "logits = model(input_data)\n",
    "predicted_probabilities = nn.Softmax(dim=1)(logits)\n",
    "y_predicted = predicted_probabilities.argmax(1)\n",
    "print(f\"Predicted class: {y_predicted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80545ca",
   "metadata": {},
   "source": [
    "### 4.3 - Model layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473ba114",
   "metadata": {},
   "source": [
    "We will take a minibatch of 3 images and pass them through the network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e3b9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_images = torch.rand(3, 28, 28)\n",
    "print(input_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5ba260",
   "metadata": {},
   "source": [
    "The `nn.Flatten` layer converts all of the 28x28 images into a contiguous array of pixel values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f63cdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = nn.Flatten()\n",
    "flattened_images = flatten(input_images)\n",
    "print(flattened_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2247ca5f",
   "metadata": {},
   "source": [
    "The `nn.Linear` layer applies a linear transformation to the input using the stored weights and biases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e18dc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_1 = nn.Linear(in_features=28 * 28, out_features=20)\n",
    "hidden_1 = layer_1(flattened_images)\n",
    "print(hidden_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77247b3",
   "metadata": {},
   "source": [
    "Non-linearity allows the model to create complex mappings between inputs and outputs. We will apply a non-linear transformation after the linear transformation. In this model, we ise the `nn.ReLU` activation, which is essentially a step function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec879a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_1_relu = nn.ReLU()(hidden_1)\n",
    "print(hidden_1_relu.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982cf396",
   "metadata": {},
   "source": [
    "`nn.Sequential` is an ordered container of modules. The data is passed through the modules in the order defined.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a7cb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_modules = nn.Sequential(flatten, layer_1, nn.ReLU(), nn.Linear(20, 10))\n",
    "input_images = torch.rand(3, 28, 28)\n",
    "logits = seq_modules(input_images)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12ecd11",
   "metadata": {},
   "source": [
    "### 4.4 - `nn.Softmax`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea517f2",
   "metadata": {},
   "source": [
    "The last linear layer of the neural network returns `logits`, raw values in the range $[-\\infty, \\infty]$. These values are passed to the `nn.Softmax` module, which scales the output to the range $[0, 1]$, representing the model's predicted probabilities. The `dim` parameter in the `nn.Softmax` module indicates the dimension along which all values must sum to 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3d211c",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "predicted_probabilities = softmax(logits)\n",
    "print(predicted_probabilities.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2305f157",
   "metadata": {},
   "source": [
    "### 4.5 - Model parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec4c5ec",
   "metadata": {},
   "source": [
    "Many layers inside a neural network are parameterized, i.e. they have weights and biases which are optimized during training. Below, we iterate over the parameters and print their shapes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75e3355",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Shape: {param.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beffd19e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5 - Automatic differentiation with `torch.autograd`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7991ba7b",
   "metadata": {},
   "source": [
    "When training neural networks, the most frequently used algorithm is back propagation. In this algorithm, parameters are adjusted according to the gradient of the loss function. PyTorch has a built-in differentiation engine called `torch.autograd`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a0b8c5",
   "metadata": {},
   "source": [
    "Import required modules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fffc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc8854f",
   "metadata": {},
   "source": [
    "We will define a simple, one-layer neural network, with inputs `x`, parameters `w`, biases `b`, and a loss function. We will also define an expected output, `y`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd21baef",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(5)\n",
    "y = torch.zeros(3)\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w) + b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70cf30a",
   "metadata": {},
   "source": [
    "### 5.1 - Tensors, functions and computational graphs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba950f2",
   "metadata": {},
   "source": [
    "`w` and `b` are parameters which we need to optimize, and so we need to compute gradients of the loss function with respect to these variables. In order to do this, we need to set the `requires_grad` property of these tensors to `True`.\n",
    "\n",
    "A function that we apply to tensors to construct a computational graph is an instance of the `Function` class. This object knows how to compute the function in the forwards direction, and how to compute its derivative in the backpropagation step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87d5183",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Gradient function for z = {z.grad_fn}\")\n",
    "print(f\"Gradient function for loss = {loss.grad_fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fe6e7c",
   "metadata": {},
   "source": [
    "### 5.2 - Computing gradients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2132bb31",
   "metadata": {},
   "source": [
    "To optimize weights in the neural network, we need to compute derivatives of the loss function. We can compute $\\frac{\\partial(loss)}{\\partial w}$ and $\\frac{\\partial(loss)}{\\partial b}$ by calling `loss.backward()`. We can retrieve the derivatives using `w.grad` and `b.grad`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c5b187",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdef5141",
   "metadata": {},
   "source": [
    "We can stop tracking computations by surrounding code with the `torch.no_grad()` block, or by using the `detach()` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b748619",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.matmul(x, w) + b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w) + b\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d025bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.matmul(x, w) + b\n",
    "z_det = z.detach()\n",
    "print(z_det.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68d39e4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6 - Optimizing model parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4886433a",
   "metadata": {},
   "source": [
    "Once we have a model and data, we can train the model (i.e. optimize the model's parameters using our data). Training is an iterative process. One iteration involves computing the output, evaluating the loss, calculating derivatives of the loss with respect to the parameters, and optimizing these parameters using gradient descent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867bc989",
   "metadata": {},
   "source": [
    "Load the prerequisite code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92644cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from send2trash import TrashPermissionError\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\", train=True, download=True, transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\", train=False, download=True, transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea30661",
   "metadata": {},
   "source": [
    "### 6.1 - Hyperparameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866f9071",
   "metadata": {},
   "source": [
    "Hyperparameters are adjustable parameters that let you control the optimization (training) process. We use the following hyperparameters:\n",
    "\n",
    "- Number of epochs - the number of times to iterate over the dataset.\n",
    "- Batch size - the number of samples propagated through the network before we update the parameters.\n",
    "- Learning rate - how much to update model parameters by after each batch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc2b01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c657d687",
   "metadata": {},
   "source": [
    "### 6.2 - The optimization loop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09399643",
   "metadata": {},
   "source": [
    "Once we have set our hyperparameters, we can train and optimize our model with an optimization loop. Each iteration of the optimization loop is called an epoch. Each epoch consists of two main parts:\n",
    "\n",
    "- The train loop - iterate over the training dataset, and optimize the parameters.\n",
    "- The test loop - iterate over the test dataset, and check if model performance is improving.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692bd029",
   "metadata": {},
   "source": [
    "### 6.3 - Loss functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4351ab49",
   "metadata": {},
   "source": [
    "A loss function measures how close the output from the model is to the target; in training, we try to minimize the loss function. Common loss functions include `nn.MSELoss` for regression tasks, and `nn.NLLLoss` for classification; `nn.CrossEntropyLoss` combines `nn.LogSoftmax` and `nn.NLLLoss`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f04c1ab",
   "metadata": {},
   "source": [
    "Initialize the loss function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf93770a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb67d4ea",
   "metadata": {},
   "source": [
    "### 6.4 - Optimizers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715f35a9",
   "metadata": {},
   "source": [
    "Optimization is when we adjust model parameters to minimize loss. The optimization logic is encapsulated in the `optimizer` object; in this example, we use the Stochastic Gradient Descent (SGD) optimization algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f3f770",
   "metadata": {},
   "source": [
    "Initialize the optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2bf2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f549dc8d",
   "metadata": {},
   "source": [
    "In the training loop, optimization happens in three steps:\n",
    "\n",
    "- Call `optimizer.zero_grad()` to reset the gradients of model parameters.\n",
    "- Backpropagate the loss, and the gradients of the loss with respect to the parameters, with `loss.backward()`.\n",
    "- Call `optimizer.step()` to adjust the parameters by the gradients collected in the backwards pass.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b09283c",
   "metadata": {},
   "source": [
    "### 6.5 - Full implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a329d7c",
   "metadata": {},
   "source": [
    "Define the train loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a34511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_function, optimizer):\n",
    "    # Set the model to training mode.\n",
    "    model.train()\n",
    "\n",
    "    size = len(dataloader.dataset)\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss.\n",
    "        prediction = model(X)\n",
    "        loss = loss_function(prediction, y)\n",
    "\n",
    "        # Perform backpropagation.\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d37aa8",
   "metadata": {},
   "source": [
    "Define the test loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e146fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_function):\n",
    "    # Set the model to evaluation mode.\n",
    "    model.eval()\n",
    "\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # torch.no_grad() ensures that no gradients are computed during testing.\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            prediction = model(X)\n",
    "            test_loss += loss_function(prediction, y).item()\n",
    "            correct += (prediction.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Accuracy: {(100*correct):>0.1f}%; average loss: {test_loss:>8f}. \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636a26fa",
   "metadata": {},
   "source": [
    "Perform the optimization loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3857f802",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n----------------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_function, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c9926c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7 - Saving and loading models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d76bc1",
   "metadata": {},
   "source": [
    "Import required modules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a6813f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9760b633",
   "metadata": {},
   "source": [
    "The learned parameters are saved in an internal state dictionary, `state_dict`. These parameters can be saved using the `torch.save()` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f7d1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.vgg16(weights=\"IMAGENET1K_V1\")\n",
    "torch.save(model.state_dict(), \"model_weights.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706d15df",
   "metadata": {},
   "source": [
    "To load model parameters, you need to first create an instance of the same model, and then load the parameters using the `load_state_dict()` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473cfa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.vgg16()\n",
    "model.load_state_dict(torch.load(\"model_weights.pth\", weights_only=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "numerical_geometry",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
